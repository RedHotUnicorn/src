---
title: 'LLMClone: –∫–∞–∫ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Å–µ–±—è –≤ Telegram / –•–∞–±—Ä'
date: 2023-12-18
src_link: https://www.notion.so/LLMClone-Telegram-bb0b5eec661845788511832704a34660
src_date: '2023-12-18 16:19:00'
gold_link: https://habr.com/ru/companies/mts_ai/articles/780404/
gold_link_hash: 7f99e705102d51058189444ab4ae32c2
tags:
- '#host_habr_com'
---

![](https://habrastorage.org/getpro/habr/upload_files/2df/68b/ecb/2df68becbaaf7ec635dc63fa7c171981.jpg)–£ –º–µ–Ω—è, –∫–∞–∫ –∏ —É –º–Ω–æ–≥–∏—Ö, –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ —á–∞—Ç–æ–≤ –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–º–µ. –ò–Ω–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ (–∞ –∏–Ω–æ–≥–¥–∞ –∏ –Ω–µ —Ö–æ—á–µ—Ç—Å—è) –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è. –ò–º–µ–Ω–Ω–æ —Ç–∞–∫ –≤–æ–∑–Ω–∏–∫–ª–∞ –∏–¥–µ—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –∫–ª–æ–Ω–∞. –í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–∞—è –∏–¥–µ—è, —Å–æ—Å—Ç–æ—è—â–∞—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –∑–∞—Ñ–∞–π–Ω—Ç—é–Ω–∏—Ç—å —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ –ª–∏—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö, –≤—ã–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –∏–∑ Telegram-—á–∞—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —Ç–∞–∫–æ–π –∫–ª–æ–Ω —Å–º–æ–∂–µ—Ç –æ–±—â–∞—Ç—å—Å—è –∑–∞ –≤–∞—Å

–í —Å—Ç–∞—Ç—å–µ –ø–æ–∫–∞–∑–∞–Ω–æ –æ–±—É—á–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–º–µ–Ω–Ω–æ –≤ –∫–æ–¥–µ (–≤ –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç—å—è—Ö –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫–ª–æ–Ω–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä [–∑–¥–µ—Å—å](https://habr.com/ru/articles/757086/), –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–∫—Ä–∏–ø—Ç—ã Lit-GPT, –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ 16 –±–∏—Ç) –∏ —Å–æ–∑–¥–∞–Ω–∞ –Ω–µ–±–æ–ª—å—à–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, —á—Ç–æ–±—ã –∫–∞–∂–¥—ã–π –º–æ–≥ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –æ–±—É—á–∏—Ç—å –∫–ª–æ–Ω–∞ –≤ –≥—É–≥–ª –∫–æ–ª–ª–∞–±–µ (–≤ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–π —Å—Ç–∞—Ç—å–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è A100 40GB). –ü—Ä–∏—è—Ç–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è!

–ú–æ–∑–≥ (—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å)
----------------------

–í –∫–∞—á–µ—Å—Ç–≤–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ "–º–æ–∑–≥–∞" –¥–ª—è –∫–ª–æ–Ω–∞ –±—ã–ª–∞ –≤—ã–±—Ä–∞–Ω–∞ decoder-only –º–æ–¥–µ–ª—å [*Mistral*](https://huggingface.co/mistralai/Mistral-7B-v0.1) —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç —É 13- –º–∏–ª–ª–∏–∞—Ä–¥–Ω–æ–π *LLaMA-2*. 

–û–¥–Ω–æ–π –∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π *Mistral-7B* —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, —á—Ç–æ –≤ –Ω–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è *SWA (sliding window attention)*, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–∞–∂–¥—ã–π —Å–ª–æ–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ 4096 —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π. –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π, –ø–æ –∫–æ—Ç–æ—Ä–æ–π —ç—Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –≤ *Mistral* —è–≤–ª—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å *O(sliding\_window.seq\_len)*. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ (–≤–º–µ—Å—Ç–µ —Å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º–∏ *FlashAttention* –∏ *xFormers)* SWA –ø–æ–∑–≤–æ–ª—è—é—Ç —É–≤–µ–ª–∏—á–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –≤ 2 —Ä–∞–∑–∞ –ø—Ä–∏ –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ 16k –∏ —Å –æ–∫–Ω–æ–º –≤ 4k —Ç–æ–∫–µ–Ω–æ–≤. 

![](https://habrastorage.org/getpro/habr/upload_files/4f2/b0a/854/4f2b0a854dbec9a17cd51d2c25649284.png)–ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–≤—è–∑–∞–Ω —Å W —Ç–æ–∫–µ–Ω–∞–º–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è (–∑–¥–µ—Å—å W = 3). –¢–æ–∫–µ–Ω—ã –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –≤–ª–∏—è—é—Ç –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞. –ù–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –ø–µ—Ä–µ–º–µ—â–∞—Ç—å—Å—è –≤–ø–µ—Ä–µ–¥ –Ω–∞ W —Ç–æ–∫–µ–Ω–æ–≤. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø–æ—Å–ª–µ k —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –ø—Ä–æ–¥–≤–∏–≥–∞—Ç—å—Å—è –≤–ø–µ—Ä–µ–¥ –Ω–∞ k √ó W —Ç–æ–∫–µ–Ω–æ–≤. 

–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
-----------------

–î–ª—è –Ω–∞—á–∞–ª–∞ –≤—ã–≥—Ä—É–∂–∞–µ–º json —Å–æ –≤—Å–µ–º–∏ –ª–∏—á–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏. –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –∑–∞–π—Ç–∏ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Advanced –∏ –Ω–∞–∂–∞—Ç—å export telegram data:

![](https://habrastorage.org/getpro/habr/upload_files/e66/b41/de4/e66b41de414fde9ee930d62a5e0316a3.png)–û—Ç–º–µ—á–∞–µ–º –Ω—É–∂–Ω–æ–µ (—Ç–æ–ª—å–∫–æ Personal chats) –∏ –≤—ã–≥—Ä—É–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ json:

![](https://habrastorage.org/getpro/habr/upload_files/667/2f3/1e0/6672f31e012a65cf83d13b822a81f54f.png)–î–∞–ª–µ–µ –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥—É —Å–æ–æ–±—â–µ–Ω–∏–π. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª–µ–º. –ù–∏–∫–Ω–µ–π–º—ã –≤—Å–µ—Ö —é–∑–µ—Ä–æ–≤ (–∫—Ä–æ–º–µ –∫–ª–æ–Ω–∏—Ä—É–µ–º–æ–π –ª–∏—á–Ω–æ—Å—Ç–∏) –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ "User":


```
import pandas as pd
from datasets import Dataset, load_from_disk

def process_chats(file_path: str) -> List[str]::
    df = pd.read_json(file_path)

    messages = []
    for sample in df["chats"]["list"]:
        for row in sample["messages"]:
            if row["text"] != '':
                username = row['from']
                if username != "Alan":
                    username = "User"
                if username == "Alan":
                    username = "Clone"  
                message = f"{username}: {row['text']}"
                messages.append(message)
    
    return messages
```
–û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–¥—Ä—è–¥ –æ—Ç –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –æ–¥–Ω–æ –±–æ–ª—å—à–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:


```
merged_messages = []
current_user = ''

for message in messages:
    if message.startswith('User:'):
        if current_user != 'User':
            current_user = 'User'
            merged_messages.append(message)
        else:
            merged_messages[-1] += '\n' + message[len('User: '):]
    else:
        if current_user != 'Clone':
            current_user = 'Clone'
            merged_messages.append(message)
        else:
            merged_messages[-1] += '\n' + message[len('Clone: '):]
```
–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –±–ª–æ–∫–∏ –∫–æ–¥–∞ –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ñ—É–Ω–∫—Ü–∏—é, –Ω–æ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å. –†–∞–∑–±–∏–≤–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ –º–µ–∂–¥—É User –∏ Clone –≤ –≥—Ä—É–ø–ø—ã –ø–æ 5 —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ Dataset:


```
size = 5
num_steps = len(merged_messages)/5
samples = ("\n".join(merged_messages[i*size:(i+1)*size]) for i in range(round(num_steps)))

df = pd.DataFrame({"prompt": samples})
dataset = Dataset.from_pandas(df)
dataset.save_to_disk("clon_conversations")

print(dataset)

# >>> Dataset({
# >>>     features: ['prompt'],
# >>>     num_rows: 2460
# >>> })

print(dataset[1602].get("prompt"))

# >>> User: –ü–æ—Å—Ç–∞—Ä–∞—é—Å—å –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è —ç—Ç–æ —É–∂–µ –¥–æ–¥–µ–ª–∞—Ç—å
# >>> Clone: –î–∞–≤–∞–π –±—Ä–∞—Ç. –ö–∞–∫ —Ç–∞–º —Å —Ç–µ—Å—Ç–∞–º–∏
# >>> User: –ú–Ω–µ —á—ë—Ç–æ –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –ø–æ–∫–∞ —á—ë –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç.
# >>> Clone: –ü–æ–º–Ω–∏—à—å –ø–∞–∫–µ—Ç—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏?
# >>> User: –ü–æ–º–Ω—é
```
–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–∑ —Å–µ–±—è –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–±–º–µ–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –º–µ–∂–¥—É —é–∑–µ—Ä–æ–º –∏ –∫–ª–æ–Ω–æ–º.

–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ int 4 —Å QLoRA
-------------------------------

–î–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 4-—Ö –±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç–æ–¥ QLoRA. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –Ω–æ —É –º–æ–¥–µ–ª–∏ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º QLoRA –Ω–µ–º–Ω–æ–≥–æ –ø–æ–¥—Ä–æ–±–Ω–µ–µ.

–¢–µ–º–∞ LoRA –º–Ω–æ–≥–æ —Ä–∞–∑ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–ª–∞—Å—å –Ω–∞ —Ö–∞–±—Ä–µ, –ø–æ—ç—Ç–æ–º—É –≤ –¥–≤—É—Ö —Å–ª–æ–≤–∞—Ö: –∫ —Å–ª–æ—è–º —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏–∫—Ä–µ–ø–ª—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞ –∏ –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –∏—Ö. –í —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ –º—ã —Ö–æ—Ç–∏–º –æ–±—É—á–∏—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –≤ 4 –±–∏—Ç –º–æ–¥–µ–ª—å, –Ω–∞ –ø–æ–º–æ—â—å –ø—Ä–∏—Ö–æ–¥–∏—Ç –º–µ—Ç–æ–¥ QLoRA.

![](https://habrastorage.org/getpro/habr/upload_files/dc9/254/995/dc9254995b75ed8c3a54ea63104801e0.png "LoRA")

LoRA

–í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–æ–º –≤ 16 –±–∏—Ç, QLoRA –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ *Llama-7B* –≤ *Google Colab.* –ú–æ–¥–µ–ª—å *–≤ —Ç–∞–∫–æ–º* —Å–ª—É—á–∞–µ –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –æ–∫–æ–ª–æ 3,5 –≥–∏–≥–∞–±–∞–π—Ç.

QLoRA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ó–∞—Ç–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç—Å—è, –∏ –≤ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –≤–∏–¥–µ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Å –Ω–∏–∑–∫–∏–º —Ä–∞–Ω–≥–æ–º. –í–æ –≤—Ä–µ–º—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ QLoRA –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—É—é 4-–±–∏—Ç–Ω—É—é –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ –∞–¥–∞–ø—Ç–µ—Ä—ã. –°–ª–æ–∏ LoRA - —ç—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.

QLoRA –∏–º–µ–µ—Ç –æ–¥–∏–Ω —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö —Ö—Ä–∞–Ω–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ 4-–±–∏—Ç–Ω—ã–π NormalFloat) –¥–ª—è –≤–µ—Å–æ–≤ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö BrainFloat 16, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. QLoRA –¥–µ–∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –æ—Ç —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä—è–º–æ–≥–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–æ–≤, –Ω–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è 16 –±–∏—Ç–Ω—ã—Ö –∞–¥–∞–ø—Ç–µ—Ä–æ–≤. –í–µ—Å–∞ –¥–µ–∫–≤–∞–Ω—Ç–∏–∑–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –æ–Ω–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º –≤–æ –≤—Ä–µ–º—è —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∑—è—Ç–∞ –∏–∑ [–±–ª–æ–≥–∞ HF](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

–ü–µ—Ä–µ–π–¥–µ–º –∫ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –Ω–∞—à–µ–≥–æ "–∫–ª–æ–Ω–∞". –î–ª—è –Ω–∞—á–∞–ª–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫ transformers –∏ peft:


```
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

from peft import (
    LoraConfig, 
    get_peft_model, 
    prepare_model_for_kbit_training,
    TaskType
    )
```
–î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –≤ 4 –±–∏—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ BitsAndBytes, –∫–æ—Ç–æ—Ä–∞—è —É–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ transformers. –ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∫–≤–∞–Ω—Ç–æ–≤–∞—Ç—å –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏:

–° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç–∞ load\_in\_4bit:


```
model = AutoModelForCausalLM.from_pretrained(checkpoint, 
                                             load_in_4bit=True,
                                             device_map="auto")
```
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (BitsAndBytesConfig):

* bnb\_4bit\_compute\_dtype. –ê—Ä–≥—É–º–µ–Ω—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–µ–Ω—è—Ç—å —Ç–∏–ø –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ bf16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è. –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ fp32
* bnb\_4bit\_quant\_type. 4-–±–∏—Ç–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å—Å—è —Å 2 —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è: FP4 –∏ NF4 (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é). –¢–∏–ø NF4 (NormalFloat 4) –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ —Å—Ç–∞—Ç—å–µ QLoRA. NormalFloat —ç—Ç–æ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å [–∑–¥–µ—Å—å](https://arxiv.org/abs/2305.14314)
* bnb\_4bit\_use\_double\_quant. –í–ª–æ–∂–µ–Ω–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞—Å—Ö–æ–¥ –ø–∞–º—è—Ç–∏ - –ø–æ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º, —ç—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞—Ñ–∞–π–Ω—Ç—é–Ω–∏—Ç—å llama-13b –Ω–∞ 16 –ì–ë NVIDIA-T4 —Å –¥–ª–∏–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 1024, —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ 1 –∏ —à–∞–≥–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (gradient accumulation) 4. –ß—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é, –¥–æ–±–∞–≤–ª—è–µ–º bnb\_4bit\_use\_double\_quant=True –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥–∞


```
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

```
–ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∫–≤–∞–Ω—Ç—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å:


```
checkpoint = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenizer.pad_token_id = tokenizer.eos_token_id
model = AutoModelForCausalLM.from_pretrained(
    checkpoint,
    quantization_config=bnb_config,
    device_map="auto"
)
```
–ó–∞—Ç–µ–º –º—ã –¥–æ–ª–∂–Ω—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∫ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –µ–µ –∫ –æ–±—É—á–µ–Ω–∏—é –≤ 4 –±–∏—Ç. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º prepare\_model\_for\_kbit\_training


```
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)
```
–ü—Ä–∏–∫—Ä–µ–ø–ª—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã


```
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=[
              "q_proj", 
              "k_proj",
              "v_proj",
              "o_proj"],
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
```
–ü–æ–¥–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º:


```
dataset = load_from_disk("clone_conversations")
dataset = dataset.map(lambda example: tokenizer(example["prompt"], max_length=256), batched=True)
dataset = dataset.train_test_split(0.1, 0.9)
```
–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º paged\_adamw\_8bit, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ gradient\_checkpointing\_enable


```
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="llama",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=2,
    logging_steps=100,
    save_steps=1000,
    learning_rate=2e-4,
    optim="paged_adamw_8bit",
    fp16=True,
    num_train_epochs=10,
    ddp_find_unused_parameters=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collator,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"]
)

trainer.train()

model.save_pretrained("clone_peft")
```
–ü—Ä–∏–º–µ—Ä –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:


```
from peft import PeftModel

bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )

model = AutoModelForCausalLM.from_pretrained(
                checkpoint, 
                quantization_config=bnb_config,
                device_map="auto"
            )

model = PeftModel.from_pretrained(model, "clone_peft")
tokenizer = AutoTokenizer.from_pretraiend(checkpoint)

def generate_sample(
            prompt,
            num_return_sequences=1,
            max_new_tokens=128,
            max_length=1024
        ):
        
        input_ids = tokenizer(
            prompt,
            max_length=max_length,
            truncation=True,
            return_tensors="pt"
        ).input_ids

        tokens = model.generate(
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            num_beams=num_return_sequences
        )

        decoded_tokens = tokenizer.decode(
            tokens[0],
            skip_special_tokens=True
        )

        return decoded_tokens

generate_sample("–ö–∞–∫ –¥–µ–ª–∞?")
# >>> –ü–æ–π–¥–µ—Ç
```
LLMClone
--------

–í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –∏–∑–ª–∏—à–Ω–µ (HF Transformers –∏ —Ç–∞–∫ —è–≤–ª—è–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π), –Ω–æ –µ—Å–ª–∏ –∫–æ–º—É- —Ç–æ —Ö–æ—á–µ—Ç—Å—è –±—ã—Å—Ç—Ä–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–æ–∑–¥–∞—Ç—å –∫–ª–æ–Ω–∞ –∏–∑ –∫–æ—Ä–æ–±–∫–∏, —è –Ω–∞–ø–∏—Å–∞–ª –Ω–µ–±–æ–ª—å—à—É—é –ª–∏–±—É [LLMClone](https://github.com/alanrbtx/llmclone)üë®üèªü§ñ, –≤ –∫–æ—Ç–æ—Ä—É—é –∑–∞–≤–µ—Ä–Ω—É—Ç –≤–µ—Å—å –∫–æ–¥ –∏–∑ —Å—Ç–∞—Ç—å–∏ (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –≤–Ω—É—Ç—Ä–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≤–Ω—É—Ç—Ä–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏), –∏ –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–∫–∏–Ω—É—Ç—å —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ —Ç–µ–ª–µ–≥–∏ –∏ –æ–±—É—á–∏—Ç—å LLaMA-7b –ø—Ä—è–º–æ –≤ —Åollab (–Ω–æ—É—Ç–±—É–∫ —Å –ø—Ä–∏–º–µ—Ä–æ–º –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è). –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã, –Ω–æ –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å–æ —Å–≤–æ–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ–∫–æ–ø–∞—Ç—å—Å—è –≤ —Ñ–∞–π–ª–∞—Ö:


```
git clone https://github.com/alanrbtx/llmclone
cd llmclone
pip install -r requirements.txt
```

```
from clone import (
    process_tg_data,
    LLMClone,
    train_clone,
    CloneConfig
)
```
–ì–æ—Ç–æ–≤–∏–º –¥–∞—Ç–∞—Å–µ—Ç:


```
dataset = process_tg_data(
    file_path="result.json",
    user="<your_user_name>"
    output_format="dataset",
)
```
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å:


```
config = CloneConfig(
    model_name="huggyllama/llama-7b",
    quantized=True
)

clone = LLMClone(config)
```
–¢—Ä–µ–Ω–∏—Ä—É–µ–º –∫–ª–æ–Ω–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç:


```
train_clone(clone, dataset)

prompt = """
User: –ß–µ–º –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è?
"""
clone.sample(prompt)
```
–¢–∞–∫–∂–µ, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–æ–Ω–∞, –º–æ–∂–Ω–æ –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—è–≤–∏—Ç—Å—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:


```
clone.add_adapters("clone_peft")
```
–ü—Ä–∏–º–µ—Ä –æ–±—â–µ–Ω–∏—è —Å –∫–ª–æ–Ω–æ–º
-----------------------

![](https://habrastorage.org/getpro/habr/upload_files/eb5/87f/2e4/eb587f2e43e259dee0956a4daf8b6d34.png)

User- Clone interaction

### –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ö–ª–æ–Ω –ø–æ–¥—Ü–µ–ø–∏–ª –º–æ–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ–±—â–µ–Ω–∏—è (—á—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ –Ω–µ—Å–ª–æ–∂–Ω–æ, —è —Å–∞–º –æ–±—â–∞—é—Å—å –∫–∞–∫ —Ä–æ–±–æ—Ç) –∏ –∑–Ω–∞–µ—Ç –∫–∞–∫—É—é- —Ç–æ —á–∞—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±–æ –º–Ω–µ (–º–æ–µ –∏–º—è, —á–µ–º —è –∑–∞–Ω–∏–º–∞–ª—Å—è –≤ –∫–∞–∫–æ–π- —Ç–æ –ø–µ—Ä–∏–æ–¥ –∂–∏–∑–Ω–∏, –º–æ–∏—Ö –¥—Ä—É–∑–µ–π, —Ñ–∞–∫—É–ª—å—Ç–µ—Ç –∏ –ø—Ä–æ—á–µ–µ). –° —ç—Ç–∏–º –∏ —Å–≤—è–∑–∞–Ω—ã –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –º–æ–∂–Ω–æ –≤—ã—É–¥–∏—Ç—å –∏–∑ –±–æ—Ç–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –†–µ—à–µ–Ω–∏–µ–º —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–±–æ—Ä –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ø–∞–¥–∞—é—â–∏—Ö –ø–æ–¥ NDA. –î–∞–ª–µ–µ –∫–ª–æ–Ω–∞ –º–æ–∂–Ω–æ –≤—Å—Ç—Ä–æ–∏—Ç—å –≤ —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞, –∏ —Ç–µ–ø–µ—Ä—å –æ–Ω –±—É–¥–µ—Ç –æ–±—â–∞—Ç—å—Å—è –≤–º–µ—Å—Ç–æ –≤–∞—Å, –∑–∞—Ç–µ–º –º–æ–∂–Ω–æ –ø—Ä–∏–∫—Ä—É—Ç–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è image captioning, —á—Ç–æ–±—ã –∫–ª–æ–Ω –º–æ–≥ –ø–æ–Ω–∏–º–∞—Ç—å –º–µ–º—ã –∏ –Ω–µ –≤—ã–∑—ã–≤–∞–ª –ø–æ–¥–æ–∑—Ä–µ–Ω–∏—è —É –¥—Ä—É–∑–µ–π –∏ –∫–æ–ª–ª–µ–≥

–í–∞—Ä–∏–∞–Ω—Ç—ã —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–æ–Ω–∞ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–µ–¥—É—é—â–∏–º–∏: 

* –°–æ–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–æ–≤.
* –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç –¥–æ–æ–±—É—á–∏—Ç—å –Ω–µ –±–∞–∑–æ–≤—É—é, –∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –∫ –∑–Ω–∞–Ω–∏—è–º –∏ —É–º–µ–Ω–∏—è–º –º–æ–¥–µ–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –ø—Ä–∏—Å—É—â–∏–µ –∫–ª–æ–Ω–∏—Ä—É–µ–º–æ–π –ª–∏—á–Ω–æ—Å—Ç–∏.
* –î—Ä—É–≥–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã —Ä–∞–∑–±–∞–≤–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–∞–º–∏ clone-user –∏ –æ–±—É—á–∏—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Å–º–µ—à–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.